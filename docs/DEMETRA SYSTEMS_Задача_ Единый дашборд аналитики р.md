<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# DEMETRA SYSTEMS

–ó–∞–¥–∞—á–∞: –ï–¥–∏–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤
–¶–µ–ª—å:¬†–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤—Å–µ—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:¬†Instagram, Google Ads, TikTok, Telegram –∏ –¥—Ä—É–≥–∏–µ
–û—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
–ì—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–Ω–∞–º–∏–∫–∞ —Ä–æ—Å—Ç–∞
–°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞—è–≤–∫–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
–û—Ö–≤–∞—Ç—ã –∏ –æ—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤
AI-—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤
–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫—É–ø–∞–µ–º–æ—Å—Ç–∏
–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –±—é–¥–∂–µ—Ç–∞
–†–µ–∑—É–ª—å—Ç–∞—Ç:¬†–ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å —á–µ—Ç–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π.
---
give me the architectural solution for this one!

–î–ª—è –∑–∞–¥–∞—á–∏ —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ DEMETRA SYSTEMS —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Instagram, Google Ads, TikTok, Telegram –∏ –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º, –≤–∫–ª—é—á–∞—è AI-—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª, –º–æ–∂–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—É—é —Å—Ö–µ–º—É.

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ –¥–∞—à–±–æ—Ä–¥–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ä–µ–∫–ª–∞–º–Ω—ã–º–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞–º–∏ Instagram, Google Ads, TikTok, Telegram –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏—è—Ö, –æ—Ö–≤–∞—Ç–∞—Ö, –∫–ª–∏–∫–∞—Ö, –∫–æ–Ω–≤–µ—Ä—Å–∏—è—Ö.
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º —á–µ—Ä–µ–∑ —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã.
- ETL-–ø—Ä–æ—Ü–µ—Å—Å—ã (Extract, Transform, Load) –¥–ª—è –æ—á–∏—Å—Ç–∫–∏, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –µ–¥–∏–Ω—É—é –±–∞–∑—É –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.


### –•—Ä–∞–Ω–∏–ª–∏—â–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

- –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö (Data Warehouse –∏–ª–∏ Data Lake) –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.
- –ö–æ–ª–æ–Ω–∫–∏ –∏ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: —Å—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞—è–≤–∫–∏, –∫–æ–Ω–≤–µ—Ä—Å–∏–∏, –æ—Ö–≤–∞—Ç—ã, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å.


### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ –¥–∞—à–±–æ—Ä–¥–µ

- –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è–º–∏ (–ª–∏–Ω–µ–π–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –¥–∏–Ω–∞–º–∏–∫–∏ —Ä–æ—Å—Ç–∞, –¥–∏–∞–≥—Ä–∞–º–º—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç, —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤).
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å drill-down –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ —Ä–µ–∫–ª–∞–º–Ω—ã–º –∫–∞–º–ø–∞–Ω–∏—è–º –∏ –∫–∞–Ω–∞–ª–∞–º.
- KPI-–ø–∞–Ω–µ–ª–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏.


### AI-—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª

- –ú–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –º–µ—Ç—Ä–∏–∫–∞–º.
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫—É–ø–∞–µ–º–æ—Å—Ç–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–π.
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±—é–¥–∂–µ—Ç–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º —Å —É—á—ë—Ç–æ–º –ø—Ä–æ–≥–Ω–æ–∑–∞ –∏ –∑–∞–¥–∞–Ω–Ω—ã—Ö –±–∏–∑–Ω–µ—Å-—Ü–µ–ª–µ–π.
- –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á—ë—Ç—ã –∏ –ø–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π.


### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å—Ç–µ–∫ (–ø—Ä–∏–º–µ—Ä)

- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: RESTful API, Webhooks, SDK.
- –•—Ä–∞–Ω–∏–ª–∏—â–µ: –æ–±–ª–∞—á–Ω—ã–µ OLAP-–±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, BigQuery, Snowflake).
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: Apache Kafka, Apache Spark.
- –ú–æ–¥–µ–ª–∏ AI: Python ML-–±–∏–±–ª–∏–æ—Ç–µ–∫–∏ (scikit-learn, TensorFlow).
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: React, D3.js, Grafana –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ BI-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.
- –û—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏—è: Apache Airflow –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ETL –∏ ML-–ø–∞–π–ø–ª–∞–π–Ω–∞–º–∏.


### –ò—Ç–æ–≥

–î–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ –µ–¥–∏–Ω–æ–º –º–µ—Å—Ç–µ, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—Ç—å –ø—Ä–æ–∑—Ä–∞—á–Ω—É—é –∞–Ω–∞–ª–∏—Ç–∏–∫—É, –ø—Ä–∏–º–µ–Ω—è—Ç—å –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –±—é–¥–∂–µ—Ç—É. –≠—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π.[^1_1][^1_2][^1_3][^1_4]
<span style="display:none">[^1_10][^1_5][^1_6][^1_7][^1_8][^1_9]</span>

<div align="center">‚ÅÇ</div>

[^1_1]: https://libril.com/blog/unified-analytics-dashboard-setup

[^1_2]: https://analyticsliv.com/blogs/track-conversions-ga4-meta-tiktok-google-ads

[^1_3]: https://www.womenconquerbiz.com/ai-simplifies-marketing-budget-planning/

[^1_4]: https://madgicx.com/blog/budget-forecasting-with-ai

[^1_5]: https://hightime.media/dashbordi-po-marketingu/

[^1_6]: https://pizza-padre.com/5-fun-pizza-options-inspired-by-the-caped-crusader/

[^1_7]: https://www.sprinklr.com/blog/unified-analytics-next-era-of-high-performance-marketing/

[^1_8]: https://docs.cloudera.com/data-warehouse/cloud/unified-analytics-overview/topics/dw-ua-architecture.html

[^1_9]: https://www.decisionfoundry.com/marketing-data/articles/master-marketing-dashboard-mastery-examples-and-templates/

[^1_10]: https://blog.mvpproject.io/guides/integration-of-tiktok-ads-and-telegram-how-to-attract-audience-and-convert-into-clients/?lang=en


---

# DEMETRA SYSTEMS

–ó–∞–¥–∞—á–∞: –ï–¥–∏–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤

–¶–µ–ª—å: –†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤—Å–µ—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ.

–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã: Instagram, Google Ads, TikTok, Telegram –∏ –¥—Ä—É–≥–∏–µ

–û—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏:

- –ì—Ä–∞—Ñ–∏–∫–∏ –∏ –¥–∏–Ω–∞–º–∏–∫–∞ —Ä–æ—Å—Ç–∞
- –°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞—è–≤–∫–∏ –∏ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
- –û—Ö–≤–∞—Ç—ã –∏ –æ—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤

AI-—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:

- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–Ω–∞–ª–æ–≤
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–∫—É–ø–∞–µ–º–æ—Å—Ç–∏
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –±—é–¥–∂–µ—Ç–∞

–†–µ–∑—É–ª—å—Ç–∞—Ç: –ë—ã—Å—Ç—Ä–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö —Å —á–µ—Ç–∫–∏–º–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π.

# Data Visualization

# API Integration

# AI Analytics

# Dashboard


---

DEMETRA SYSTEMS

–ó–∞–¥–∞—á–∞: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ AI-–¥–∞—à–±–æ—Ä–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π

üéØ –ö–µ–π—Å–æ–≤–∞—è –∑–∞–¥–∞—á–∞:

–ö–æ–º–ø–∞–Ω–∏—è DEMETRA SYSTEMS –≤–µ–¥–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –∫–∞–º–ø–∞–Ω–∏–∏ –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ—Ç—è—Ö –∏ digital-–∫–∞–Ω–∞–ª–∞—Ö. –í –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –µ–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π, —á—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–≥–æ –±—é–¥–∂–µ—Ç–∞ –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π.

üìã –ó–∞–¥–∞—á–∞:

–†–∞–∑—Ä–∞–±–æ—Ç–∞—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.

üéØ –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:

- –ú—É–ª—å—Ç–∏–∫–∞–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ (Instagram, Facebook, TikTok, Google Ads, –Ø–Ω–¥–µ–∫—Å.–î–∏—Ä–µ–∫—Ç –∏ –¥—Ä.)
- AI-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- –ü—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ –±—é–¥–∂–µ—Ç–∞
- –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–∞–∑–Ω—ã—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç—á–µ—Ç—ã: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏

üöÄ –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:

- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ROI –ø–æ –∫–∞–Ω–∞–ª–∞–º
- AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é –±—é–¥–∂–µ—Ç–∞
- –ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
- –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–∞–º–ø–∞–Ω–∏–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞
- –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–π

üìä –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:

–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –≤–µ–±-–¥–∞—à–±–æ—Ä–¥ —Å AI-–∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤–æ–π –∫–æ–º–∞–Ω–¥–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è data-driven —Ä–µ—à–µ–Ω–∏–π –∏ —É–≤–µ–ª–∏—á–µ–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏–π –Ω–∞ 25-40%.

üõ† –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:

- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å API –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º
- –†–µ–∞–ª–∏–∑–∞—Ü–∏—è machine learning –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö
- –°–∏—Å—Ç–µ–º–∞ –æ–ø–æ–≤–µ—â–µ–Ω–∏–π –æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π

Data Analytics AI/ML Dashboard Digital Marketing API Integration Data Visualization

---

—ç—Ç–æ —Å–≤–µ—Ä—Ö—É –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–∞–º–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞,
–ø–æ–º–æ–≥–∏ –º–Ω–µ –≤—ã—Å—Ç—Ä–æ–∏—Ç—å –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É!

—É –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å:

- backend (fastapi, postgres) initial setup
- frontend (react) initial setup
- docker compose for front/back/db
- ML model
- LLM model

image.jpg - —ç—Ç–æ csv —Ñ–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –º—ã –æ–±—É—á–∞–µ–º ML model –≤ –≤–∏–¥–µ ([Social Media Advertising Dataset | Kaggle](https://www.kaggle.com/datasets/jsonk11/social-media-advertising-dataset))
ml.ipynb - —ç—Ç–æ ml model –∏ –¥–∞—à–±–æ—Ä–¥—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑—É—è Saeborn (–∏—Å–ø–æ–ª—å–∑—É—è –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏–µ –∏ Booster)
recommendations.py - —ç—Ç–æ LLM –∫–æ—Ç–æ—Ä—ã–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–µ–Ω –±—É–¥–µ—Ç –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –≤–∏–¥–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å —á—Ç–æ —Ç–æ!

---

—ç—Ç–æ –≤—Å–µ –≤–µ—â–∏ –¥–æ–ª–∂–Ω—ã —Ç–∏–ø–∞ –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –≤ —á–∞—Å!

—Ç–∞–∫ –∂–µ –∫–∞–∫–∏–µ —É–ª—É—á—à–µ–Ω–∏–µ —Ç—ã –º–æ–∂–µ—à—å –¥–æ–±–∞–≤–∏—Ç—å –ø–æ —Ç–∏–ø—É –∞–ª–µ—Ä—Ç–æ–≤?

---

–¶–ï–õ–¨: —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å MVP MVP –ø—Ä–æ–µ–∫—Ç –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å –∫–∞–∫ —Ç–æ —á—Ç–æ –º—ã —Ä–µ—à–∏–ª–∏ —ç—Ç–æ—Ç –∫–µ–π—Å –≤ —Å–∞–º–æ–º —Ö–∞–∫–∞—Ç–æ–Ω–µ!
–∏ —Ü–µ–ª—å —ç—Ç–æ —Å–æ–∑–¥–∞—Ç—å –∏ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞—Ç—å —ç—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç!!!
–ø–æ–º–æ–≥–∏ –º–Ω–µ —Å —ç—Ç–∏–º, —É –º–µ–Ω—è –∫—Ä–∞—Ç–∫–∏–µ —Å—Ä–æ–∫–∏ –∏ –º–Ω–µ –Ω—É–∂–Ω–æ —á—Ç–æ —Ç–æ, —Å –∫–æ–º–∞–Ω–¥–æ–π –¥–∞ —Å–¥–µ–ª–∞—Ç—å!!!

–û—Ç–ª–∏—á–Ω–æ! –ü–æ–Ω—è–ª —Å–∏—Ç—É–∞—Ü–∏—é. –£ –≤–∞—Å –∫—Ä–∞—Ç–∫–∏–µ —Å—Ä–æ–∫–∏ –∏ –Ω—É–∂–Ω–æ –±—ã—Å—Ç—Ä–æ —Å–æ–±—Ä–∞—Ç—å MVP –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞. –î–∞–≤–∞–π—Ç–µ –ø–æ—Å—Ç—Ä–æ–∏–º **–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É**, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∑–∞ –∫–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è.

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ MVP –¥–ª—è DEMETRA SYSTEMS

### **–û–±—â–∞—è —Å—Ö–µ–º–∞ —Å–∏—Å—Ç–µ–º—ã**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FRONTEND (React)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Dashboard  ‚îÇ  ‚îÇ  Analytics   ‚îÇ  ‚îÇ   AI Recommendations    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   View     ‚îÇ  ‚îÇ  Charts      ‚îÇ  ‚îÇ        Panel            ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ REST API
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      BACKEND (FastAPI)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ                   API Endpoints                          ‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ
‚îÇ  ‚îÇ  /api/campaigns  ‚îÇ  /api/analytics  ‚îÇ  /api/predictions ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  /api/recommendations  ‚îÇ  /api/alerts  ‚îÇ  /api/refresh  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                      ‚îÇ                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ    BUSINESS LOGIC LAYER                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Campaign Service  ‚Ä¢ Analytics Service                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ML Prediction Service  ‚Ä¢ LLM Recommendation Service   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Alert Service  ‚Ä¢ Data Aggregation Service             ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL ‚îÇ  ‚îÇ  ML Model    ‚îÇ  ‚îÇ  LLM Model   ‚îÇ
‚îÇ   Database   ‚îÇ  ‚îÇ  (Pickle)    ‚îÇ  ‚îÇ (HuggingFace)‚îÇ
‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ ‚Ä¢ Campaigns  ‚îÇ  ‚îÇ HistGradient ‚îÇ  ‚îÇ Qwen2.5-7B   ‚îÇ
‚îÇ ‚Ä¢ Metrics    ‚îÇ  ‚îÇ   Boosting   ‚îÇ  ‚îÇ  Instruct    ‚îÇ
‚îÇ ‚Ä¢ Predictions‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ ‚Ä¢ Alerts     ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              BACKGROUND SCHEDULER                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   APScheduler / Celery + Redis (optional)          ‚îÇ ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Hourly data refresh                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ML model predictions update                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ LLM recommendations generation                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Alert checks and notifications                  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```


***

## üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è)

```
demetra-dashboard/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI app entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                  # Settings, env variables
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py                # DB connection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ campaign.py            # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metric.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prediction.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alert.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ campaign.py            # Pydantic schemas
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recommendation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ campaigns.py           # Campaign CRUD endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics.py           # Analytics endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictions.py         # ML predictions endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recommendations.py     # LLM recommendations
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alerts.py              # Alert endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ campaign_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analytics_service.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_service.py          # ML model integration
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py         # LLM integration (recommendations.py)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alert_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.pkl              # Trained ML model
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predictor.py           # ML prediction logic
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ features.py            # Feature engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scheduler/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tasks.py               # Scheduled tasks (hourly refresh)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ helpers.py
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dashboard.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ChannelComparison.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ROIChart.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MetricsCards.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AIRecommendations.jsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ AlertPanel.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api.js                 # API client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.jsx
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .env
‚îî‚îÄ‚îÄ README.md
```


***

## üóÑÔ∏è –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö PostgreSQL (—Å—Ö–µ–º–∞)

```sql
-- Campaigns table
CREATE TABLE campaigns (
    id SERIAL PRIMARY KEY,
    campaign_id VARCHAR(50) UNIQUE NOT NULL,
    target_audience VARCHAR(100),
    campaign_goal VARCHAR(100),
    duration VARCHAR(50),
    channel_used VARCHAR(50),
    conversion_rate FLOAT,
    acquisition_cost FLOAT,
    roi FLOAT,
    location VARCHAR(100),
    language VARCHAR(50),
    clicks INTEGER,
    impressions INTEGER,
    engagement_score INTEGER,
    customer_segment VARCHAR(100),
    date DATE,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- ML Predictions table
CREATE TABLE predictions (
    id SERIAL PRIMARY KEY,
    campaign_id VARCHAR(50) REFERENCES campaigns(campaign_id),
    predicted_roi FLOAT,
    confidence_score FLOAT,
    risk_level VARCHAR(20),
    created_at TIMESTAMP DEFAULT NOW()
);

-- LLM Recommendations table
CREATE TABLE recommendations (
    id SERIAL PRIMARY KEY,
    recommendation_text TEXT,
    channel_comparison JSONB,
    budget_allocation JSONB,
    summary TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Alerts table
CREATE TABLE alerts (
    id SERIAL PRIMARY KEY,
    alert_type VARCHAR(50),        -- 'roi_drop', 'cost_spike', 'low_conversion', 'anomaly'
    severity VARCHAR(20),           -- 'low', 'medium', 'high', 'critical'
    channel VARCHAR(50),
    message TEXT,
    metric_value FLOAT,
    threshold_value FLOAT,
    is_read BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT NOW()
);

-- Analytics Cache (hourly aggregations)
CREATE TABLE analytics_cache (
    id SERIAL PRIMARY KEY,
    channel VARCHAR(50),
    total_spend FLOAT,
    total_roi FLOAT,
    avg_conversion_rate FLOAT,
    total_clicks INTEGER,
    total_impressions INTEGER,
    period_start TIMESTAMP,
    period_end TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```


***

## üîÑ Backend Services (–∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)

### **1. ML Service (`services/ml_service.py`)**

```python
import pickle
import pandas as pd
import numpy as np
from typing import Dict, List

class MLService:
    def __init__(self, model_path: str = "app/ml/model.pkl"):
        with open(model_path, 'rb') as f:
            self.model = pickle.load(f)
    
    def prepare_features(self, campaign_data: Dict) -> pd.DataFrame:
        """Prepare features for ML model"""
        df = pd.DataFrame([campaign_data])
        
        # Feature engineering (based on your ml.ipynb)
        df['CTR'] = df['Clicks'] / df['Impressions']
        df['Engagement_Rate'] = df['Engagement_Score'] / df['Impressions']
        df['log_Impressions'] = np.log1p(df['Impressions'])
        df['log_Clicks'] = np.log1p(df['Clicks'])
        df['log_Engagement_Score'] = np.log1p(df['Engagement_Score'])
        
        # Extract date features
        df['Date'] = pd.to_datetime(df['Date'])
        df['Year'] = df['Date'].dt.year
        df['Month'] = df['Date'].dt.month
        df['DayOfWeek'] = df['Date'].dt.dayofweek
        
        return df
    
    def predict_roi(self, campaign_data: Dict) -> Dict:
        """Predict ROI for campaign"""
        features = self.prepare_features(campaign_data)
        prediction = self.model.predict(features)[^2_0]
        
        # Calculate confidence and risk level
        confidence = min(0.95, max(0.5, 1 - abs(prediction - campaign_data.get('roi', prediction)) / 10))
        risk_level = self._calculate_risk_level(prediction, campaign_data)
        
        return {
            'predicted_roi': float(prediction),
            'confidence_score': float(confidence),
            'risk_level': risk_level,
            'current_roi': campaign_data.get('roi'),
            'roi_difference': float(prediction - campaign_data.get('roi', 0))
        }
    
    def _calculate_risk_level(self, predicted_roi: float, campaign_data: Dict) -> str:
        """Calculate risk level based on prediction"""
        if predicted_roi < 1.0:
            return 'high'
        elif predicted_roi < 2.0:
            return 'medium'
        else:
            return 'low'
    
    def batch_predict(self, campaigns: List[Dict]) -> List[Dict]:
        """Predict ROI for multiple campaigns"""
        return [self.predict_roi(campaign) for campaign in campaigns]
```


### **2. LLM Service (`services/llm_service.py`)**

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞—à `recommendations.py` –∫–∞–∫ –±–∞–∑—É:

```python
import os
import json
import requests
from typing import Dict, Any
from dotenv import load_dotenv

load_dotenv()

class LLMService:
    def __init__(self):
        self.hf_api_key = os.getenv("HF_API_KEY")
        self.hf_api_url = "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-7B-Instruct"
        
        self.prompt_template = """
You are an analytics assistant for a marketing dashboard.
You ALWAYS return a strictly valid JSON object.

The user will provide:
- Aggregated analytics data from ad platforms
- ML model results (ROI drivers, forecasts, risk segments)

Your tasks:
1. Compare performance across channels.
2. Identify strengths, weaknesses, anomalies.
3. Analyze ML feature importance.
4. Give recommendations for:
   - budget allocation
   - CPC / CTR / CPM improvements
   - conversion optimization
   - ROI growth
5. Provide a short summary.

Your JSON response MUST follow this structure:
{
  "channel_comparison": {},
  "ml_predictions": {},
  "recommendations": {},
  "summary": ""
}

Rules:
- STRICTLY return valid JSON.
- NO extra text.
- Use ONLY metrics provided by the user.
- Respond in English.
"""
    
    def generate_recommendations(self, analytics_data: Dict, ml_predictions: Dict) -> Dict:
        """Generate AI recommendations based on analytics and ML predictions"""
        
        user_data = {
            "analytics": analytics_data,
            "ml_predictions": ml_predictions
        }
        
        user_prompt = json.dumps(user_data, indent=2)
        
        headers = {
            "Authorization": f"Bearer {self.hf_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "inputs": f"{self.prompt_template}\n\nUSER DATA:\n{user_prompt}",
            "parameters": {
                "temperature": 0.2,
                "max_new_tokens": 700
            }
        }
        
        try:
            response = requests.post(self.hf_api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if isinstance(data, list) and "generated_text" in data[^2_0]:
                generated = data[^2_0]["generated_text"]
                # Extract JSON from response
                json_start = generated.find('{')
                json_end = generated.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    return json.loads(generated[json_start:json_end])
            
            return {"error": "Failed to parse LLM response"}
        
        except Exception as e:
            return {"error": str(e)}
```


### **3. Alert Service (`services/alert_service.py`)**

```python
from typing import List, Dict
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from app.models.alert import Alert
from app.models.campaign import Campaign

class AlertService:
    
    THRESHOLDS = {
        'roi_drop': 1.5,           # ROI below 1.5
        'cost_spike': 600,         # Acquisition cost above $600
        'low_conversion': 0.05,    # Conversion rate below 5%
        'high_ctr_low_conversion': {'ctr': 0.15, 'conversion': 0.05},
        'engagement_drop': 3       # Engagement score below 3
    }
    
    def check_alerts(self, db: Session) -> List[Dict]:
        """Check for various alert conditions"""
        alerts = []
        
        # Get recent campaigns (last 24 hours)
        recent_campaigns = db.query(Campaign).filter(
            Campaign.updated_at >= datetime.now() - timedelta(hours=24)
        ).all()
        
        for campaign in recent_campaigns:
            # Check ROI drop
            if campaign.roi < self.THRESHOLDS['roi_drop']:
                alerts.append(self._create_alert(
                    alert_type='roi_drop',
                    severity='high',
                    channel=campaign.channel_used,
                    message=f"ROI dropped to {campaign.roi:.2f} for {campaign.channel_used}",
                    metric_value=campaign.roi,
                    threshold_value=self.THRESHOLDS['roi_drop']
                ))
            
            # Check cost spike
            if campaign.acquisition_cost > self.THRESHOLDS['cost_spike']:
                alerts.append(self._create_alert(
                    alert_type='cost_spike',
                    severity='medium',
                    channel=campaign.channel_used,
                    message=f"Acquisition cost spiked to ${campaign.acquisition_cost} for {campaign.channel_used}",
                    metric_value=campaign.acquisition_cost,
                    threshold_value=self.THRESHOLDS['cost_spike']
                ))
            
            # Check low conversion
            if campaign.conversion_rate < self.THRESHOLDS['low_conversion']:
                alerts.append(self._create_alert(
                    alert_type='low_conversion',
                    severity='medium',
                    channel=campaign.channel_used,
                    message=f"Low conversion rate {campaign.conversion_rate:.2%} for {campaign.channel_used}",
                    metric_value=campaign.conversion_rate,
                    threshold_value=self.THRESHOLDS['low_conversion']
                ))
            
            # Check anomaly: high CTR but low conversion
            ctr = campaign.clicks / campaign.impressions if campaign.impressions > 0 else 0
            if (ctr > self.THRESHOLDS['high_ctr_low_conversion']['ctr'] and 
                campaign.conversion_rate < self.THRESHOLDS['high_ctr_low_conversion']['conversion']):
                alerts.append(self._create_alert(
                    alert_type='anomaly',
                    severity='high',
                    channel=campaign.channel_used,
                    message=f"High CTR ({ctr:.2%}) but low conversion ({campaign.conversion_rate:.2%}) - possible landing page issue",
                    metric_value=campaign.conversion_rate,
                    threshold_value=self.THRESHOLDS['high_ctr_low_conversion']['conversion']
                ))
            
            # Check engagement drop
            if campaign.engagement_score < self.THRESHOLDS['engagement_drop']:
                alerts.append(self._create_alert(
                    alert_type='engagement_drop',
                    severity='low',
                    channel=campaign.channel_used,
                    message=f"Low engagement score {campaign.engagement_score} for {campaign.channel_used}",
                    metric_value=campaign.engagement_score,
                    threshold_value=self.THRESHOLDS['engagement_drop']
                ))
        
        # Save alerts to database
        for alert_data in alerts:
            alert = Alert(**alert_data)
            db.add(alert)
        
        db.commit()
        return alerts
    
    def _create_alert(self, alert_type: str, severity: str, channel: str, 
                     message: str, metric_value: float, threshold_value: float) -> Dict:
        """Create alert dictionary"""
        return {
            'alert_type': alert_type,
            'severity': severity,
            'channel': channel,
            'message': message,
            'metric_value': metric_value,
            'threshold_value': threshold_value,
            'is_read': False,
            'created_at': datetime.now()
        }
    
    def get_unread_alerts(self, db: Session) -> List[Alert]:
        """Get all unread alerts"""
        return db.query(Alert).filter(Alert.is_read == False).order_by(Alert.created_at.desc()).all()
    
    def mark_alert_read(self, db: Session, alert_id: int):
        """Mark alert as read"""
        alert = db.query(Alert).filter(Alert.id == alert_id).first()
        if alert:
            alert.is_read = True
            db.commit()
```


### **4. Scheduler (`scheduler/tasks.py`)**

–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ APScheduler –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã:

```python
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from sqlalchemy.orm import Session
from app.database import SessionLocal
from app.services.ml_service import MLService
from app.services.llm_service import LLMService
from app.services.alert_service import AlertService
from app.services.analytics_service import AnalyticsService
import logging

logger = logging.getLogger(__name__)

def hourly_refresh_task():
    """Task that runs every hour to refresh predictions and recommendations"""
    logger.info("Starting hourly refresh task...")
    
    db: Session = SessionLocal()
    
    try:
        # 1. Refresh ML predictions
        ml_service = MLService()
        analytics_service = AnalyticsService()
        
        campaigns = analytics_service.get_recent_campaigns(db)
        predictions = ml_service.batch_predict(campaigns)
        analytics_service.save_predictions(db, predictions)
        
        logger.info(f"Updated {len(predictions)} ML predictions")
        
        # 2. Generate LLM recommendations
        llm_service = LLMService()
        analytics_data = analytics_service.get_aggregated_analytics(db)
        ml_predictions_data = analytics_service.get_latest_predictions(db)
        
        recommendations = llm_service.generate_recommendations(analytics_data, ml_predictions_data)
        analytics_service.save_recommendations(db, recommendations)
        
        logger.info("Generated new LLM recommendations")
        
        # 3. Check for alerts
        alert_service = AlertService()
        alerts = alert_service.check_alerts(db)
        
        logger.info(f"Generated {len(alerts)} new alerts")
        
    except Exception as e:
        logger.error(f"Error in hourly refresh task: {str(e)}")
    
    finally:
        db.close()

def start_scheduler():
    """Start the background scheduler"""
    scheduler = BackgroundScheduler()
    
    # Run every hour
    scheduler.add_job(
        func=hourly_refresh_task,
        trigger=IntervalTrigger(hours=1),
        id='hourly_refresh',
        name='Hourly data refresh and predictions',
        replace_existing=True
    )
    
    # Run immediately on startup
    scheduler.add_job(
        func=hourly_refresh_task,
        id='startup_refresh',
        name='Startup refresh',
        replace_existing=True
    )
    
    scheduler.start()
    logger.info("Scheduler started successfully")
```


***

## üé® Frontend Components (React)

### **Dashboard Layout**

```jsx
// src/components/Dashboard.jsx
import React, { useEffect, useState } from 'react';
import { api } from '../services/api';
import MetricsCards from './MetricsCards';
import ChannelComparison from './ChannelComparison';
import ROIChart from './ROIChart';
import AIRecommendations from './AIRecommendations';
import AlertPanel from './AlertPanel';

function Dashboard() {
  const [analytics, setAnalytics] = useState(null);
  const [recommendations, setRecommendations] = useState(null);
  const [alerts, setAlerts] = useState([]);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    fetchData();
    // Refresh every 5 minutes
    const interval = setInterval(fetchData, 5 * 60 * 1000);
    return () => clearInterval(interval);
  }, []);

  const fetchData = async () => {
    try {
      const [analyticsRes, recommendationsRes, alertsRes] = await Promise.all([
        api.getAnalytics(),
        api.getRecommendations(),
        api.getAlerts()
      ]);
      
      setAnalytics(analyticsRes.data);
      setRecommendations(recommendationsRes.data);
      setAlerts(alertsRes.data);
    } catch (error) {
      console.error('Error fetching data:', error);
    } finally {
      setLoading(false);
    }
  };

  if (loading) return <div>Loading...</div>;

  return (
    <div className="dashboard">
      <h1>DEMETRA SYSTEMS - AI Analytics Dashboard</h1>
      
      <AlertPanel alerts={alerts} />
      
      <MetricsCards analytics={analytics} />
      
      <div className="charts-row">
        <ChannelComparison data={analytics.channels} />
        <ROIChart data={analytics.roi_trend} />
      </div>
      
      <AIRecommendations recommendations={recommendations} />
    </div>
  );
}

export default Dashboard;
```


***

## üöÄ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è MVP

### **1. –°–∏—Å—Ç–µ–º–∞ –∞–ª–µ—Ä—Ç–æ–≤ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è)**

**–¢–∏–ø—ã –∞–ª–µ—Ä—Ç–æ–≤:**


| –¢–∏–ø –∞–ª–µ—Ä—Ç–∞ | –£—Å–ª–æ–≤–∏–µ | Severity | –î–µ–π—Å—Ç–≤–∏–µ |
| :-- | :-- | :-- | :-- |
| `roi_drop` | ROI < 1.5 | High | –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ |
| `cost_spike` | Cost > \$600 | Medium | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –±—é–¥–∂–µ—Ç |
| `low_conversion` | CR < 5% | Medium | –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞–º–ø–∞–Ω–∏—é |
| `anomaly_high_ctr_low_conv` | CTR > 15% AND CR < 5% | High | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å landing page |
| `engagement_drop` | Score < 3 | Low | –£–ª—É—á—à–∏—Ç—å –∫—Ä–µ–∞—Ç–∏–≤—ã |
| `budget_overrun` | Spend > 90% of budget | Critical | –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞–º–ø–∞–Ω–∏—é |
| `channel_underperform` | Channel ROI < avg ROI * 0.7 | Medium | –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –±—é–¥–∂–µ—Ç |

### **2. Real-time —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è (Websockets - –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)**

–î–ª—è MVP –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π polling (–∫–∞–∂–¥—ã–µ 30 —Å–µ–∫), –Ω–æ –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–Ω–∞ - WebSockets:

```python
# backend/app/api/websockets.py
from fastapi import WebSocket, WebSocketDisconnect
from typing import List

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    
    async def broadcast_alert(self, alert: dict):
        for connection in self.active_connections:
            await connection.send_json(alert)

manager = ConnectionManager()

@app.websocket("/ws/alerts")
async def websocket_endpoint(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        manager.disconnect(websocket)
```


### **3. –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–æ–≤ (–¥–ª—è –¥–µ–º–æ)**

```python
# backend/app/api/reports.py
from fastapi import APIRouter
from fastapi.responses import FileResponse
import pandas as pd
from datetime import datetime

router = APIRouter()

@router.get("/export/pdf")
async def export_pdf_report(db: Session = Depends(get_db)):
    """Generate PDF report (use reportlab or weasyprint)"""
    # Implementation...
    pass

@router.get("/export/excel")
async def export_excel_report(db: Session = Depends(get_db)):
    """Generate Excel report"""
    analytics = get_analytics_data(db)
    df = pd.DataFrame(analytics)
    
    filename = f"demetra_report_{datetime.now().strftime('%Y%m%d')}.xlsx"
    filepath = f"/tmp/{filename}"
    
    df.to_excel(filepath, index=False)
    return FileResponse(filepath, filename=filename)
```


### **4. –î–∞—à–±–æ—Ä–¥ –º–µ—Ç—Ä–∏–∫–∏ (KPI cards)**

```jsx
// src/components/MetricsCards.jsx
function MetricsCards({ analytics }) {
  const metrics = [
    {
      title: 'Total ROI',
      value: analytics.total_roi.toFixed(2),
      change: analytics.roi_change,
      trend: analytics.roi_change > 0 ? 'up' : 'down'
    },
    {
      title: 'Avg Conversion Rate',
      value: `${(analytics.avg_conversion_rate * 100).toFixed(2)}%`,
      change: analytics.conversion_change,
      trend: analytics.conversion_change > 0 ? 'up' : 'down'
    },
    {
      title: 'Total Clicks',
      value: analytics.total_clicks.toLocaleString(),
      change: analytics.clicks_change,
      trend: analytics.clicks_change > 0 ? 'up' : 'down'
    },
    {
      title: 'Avg Cost per Acquisition',
      value: `$${analytics.avg_acquisition_cost.toFixed(2)}`,
      change: analytics.cost_change,
      trend: analytics.cost_change < 0 ? 'up' : 'down' // lower is better
    }
  ];

  return (
    <div className="metrics-grid">
      {metrics.map((metric, index) => (
        <div key={index} className="metric-card">
          <h3>{metric.title}</h3>
          <div className="metric-value">{metric.value}</div>
          <div className={`metric-change ${metric.trend}`}>
            {metric.trend === 'up' ? '‚Üë' : '‚Üì'} {Math.abs(metric.change).toFixed(1)}%
          </div>
        </div>
      ))}
    </div>
  );
}
```


***

## ‚ö° –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã)

### **–î–µ–Ω—å 1: Core Backend + DB**

1. ‚úÖ Setup FastAPI structure
2. ‚úÖ Create database models and migrations
3. ‚úÖ Implement CRUD endpoints for campaigns
4. ‚úÖ Integrate ML model (pickle load + predict endpoint)
5. ‚úÖ Basic analytics aggregation endpoint

### **–î–µ–Ω—å 2: AI Integration + Frontend**

1. ‚úÖ Integrate LLM service (recommendations.py)
2. ‚úÖ Implement alert service with basic rules
3. ‚úÖ Setup React dashboard layout
4. ‚úÖ Create chart components (use recharts or chart.js)
5. ‚úÖ Connect frontend to backend API

### **–î–µ–Ω—å 3: Scheduler + Polish**

1. ‚úÖ Implement APScheduler for hourly tasks
2. ‚úÖ Add alert panel in frontend
3. ‚úÖ Add AI recommendations panel
4. ‚úÖ Styling and UX improvements
5. ‚úÖ Testing and bug fixes
6. ‚úÖ Prepare demo data and presentation

***

## üéØ MVP Features Checklist

**Must Have (P0):**

- [x] Display campaign data from CSV/DB
- [x] ML predictions for ROI
- [x] LLM recommendations
- [x] Basic charts (ROI by channel, trends)
- [x] Alert system (3-5 alert types)
- [x] Hourly refresh scheduler

**Should Have (P1):**

- [ ] Channel comparison view
- [ ] Budget allocation suggestions
- [ ] Export to Excel
- [ ] Alert notifications in UI

**Nice to Have (P2):**

- [ ] WebSocket real-time updates
- [ ] Advanced filtering
- [ ] Custom date ranges
- [ ] PDF reports

***

–ù—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —á–∞—Å—Ç—å—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏? –ú–æ–≥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–¥ –¥–ª—è –ª—é–±–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞!
<span style="display:none">[^2_1][^2_2][^2_3]</span>

<div align="center">‚ÅÇ</div>

[^2_1]: image.jpg

[^2_2]: recommendations.py

[^2_3]: ml.ipynb


---

# Backend MVP Requirements - DEMETRA SYSTEMS

–ò—Å—Ö–æ–¥—è –∏–∑ –≤–∞—à–µ–π —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞, –≤–æ—Ç **–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è MVP** —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –±—ã—Å—Ç—Ä—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞.

***

## üéØ Core MVP Backend Features (Must Have)

### **1. Database Models** (`app/models/`)

–°–æ–∑–¥–∞–π—Ç–µ 4 –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–∑–∞–º–µ–Ω–∏—Ç–µ —Ç–µ–∫—É—â—É—é `book.py`):

#### `campaign.py`

```python
from sqlalchemy import Column, Integer, String, Float, Date, DateTime
from sqlalchemy.sql import func
from app.configs.database import Base

class Campaign(Base):
    __tablename__ = "campaigns"
    
    id = Column(Integer, primary_key=True, index=True)
    campaign_id = Column(String(50), unique=True, index=True)
    target_audience = Column(String(100))
    campaign_goal = Column(String(100))
    duration = Column(String(50))
    channel_used = Column(String(50), index=True)
    conversion_rate = Column(Float)
    acquisition_cost = Column(Float)
    roi = Column(Float)
    location = Column(String(100))
    language = Column(String(50))
    clicks = Column(Integer)
    impressions = Column(Integer)
    engagement_score = Column(Integer)
    customer_segment = Column(String(100))
    date = Column(Date)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
```


#### `prediction.py`

```python
from sqlalchemy import Column, Integer, String, Float, ForeignKey, DateTime
from sqlalchemy.sql import func
from app.configs.database import Base

class Prediction(Base):
    __tablename__ = "predictions"
    
    id = Column(Integer, primary_key=True, index=True)
    campaign_id = Column(String(50), ForeignKey("campaigns.campaign_id"))
    predicted_roi = Column(Float)
    confidence_score = Column(Float)
    risk_level = Column(String(20))  # 'low', 'medium', 'high'
    feature_importance = Column(String(500))  # JSON string
    created_at = Column(DateTime(timezone=True), server_default=func.now())
```


#### `recommendation.py`

```python
from sqlalchemy import Column, Integer, Text, DateTime
from sqlalchemy.sql import func
from sqlalchemy.dialects.postgresql import JSONB
from app.configs.database import Base

class Recommendation(Base):
    __tablename__ = "recommendations"
    
    id = Column(Integer, primary_key=True, index=True)
    channel_comparison = Column(JSONB)
    ml_predictions = Column(JSONB)
    recommendations = Column(JSONB)
    summary = Column(Text)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
```


#### `alert.py`

```python
from sqlalchemy import Column, Integer, String, Float, Boolean, Text, DateTime
from sqlalchemy.sql import func
from app.configs.database import Base

class Alert(Base):
    __tablename__ = "alerts"
    
    id = Column(Integer, primary_key=True, index=True)
    alert_type = Column(String(50))  # 'roi_drop', 'cost_spike', etc.
    severity = Column(String(20))    # 'low', 'medium', 'high', 'critical'
    channel = Column(String(50))
    message = Column(Text)
    metric_value = Column(Float)
    threshold_value = Column(Float)
    is_read = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
```


***

### **2. Pydantic Schemas** (`app/schemas/`)

#### `campaign.py`

```python
from pydantic import BaseModel
from datetime import date, datetime
from typing import Optional

class CampaignBase(BaseModel):
    campaign_id: str
    target_audience: Optional[str] = None
    campaign_goal: Optional[str] = None
    duration: Optional[str] = None
    channel_used: str
    conversion_rate: float
    acquisition_cost: float
    roi: float
    location: Optional[str] = None
    language: Optional[str] = None
    clicks: int
    impressions: int
    engagement_score: int
    customer_segment: Optional[str] = None
    date: date

class CampaignCreate(CampaignBase):
    pass

class CampaignResponse(CampaignBase):
    id: int
    created_at: datetime
    updated_at: Optional[datetime] = None
    
    class Config:
        from_attributes = True

class ChannelStats(BaseModel):
    channel: str
    total_campaigns: int
    avg_roi: float
    avg_conversion_rate: float
    total_clicks: int
    total_impressions: int
    avg_acquisition_cost: float
```


#### `analytics.py`

```python
from pydantic import BaseModel
from typing import List, Dict

class AnalyticsSummary(BaseModel):
    total_campaigns: int
    total_roi: float
    avg_roi: float
    avg_conversion_rate: float
    total_clicks: int
    total_impressions: int
    avg_acquisition_cost: float
    roi_change: float  # % change
    conversion_change: float
    clicks_change: float
    cost_change: float
    channels: List[Dict]
    top_performing_channel: str
    worst_performing_channel: str

class ROITrend(BaseModel):
    date: str
    roi: float
    channel: str
```


#### `prediction.py`

```python
from pydantic import BaseModel
from datetime import datetime
from typing import Optional, Dict

class PredictionBase(BaseModel):
    campaign_id: str
    predicted_roi: float
    confidence_score: float
    risk_level: str
    feature_importance: Optional[Dict] = None

class PredictionResponse(PredictionBase):
    id: int
    created_at: datetime
    
    class Config:
        from_attributes = True
```


#### `recommendation.py`

```python
from pydantic import BaseModel
from datetime import datetime
from typing import Dict, Optional

class RecommendationResponse(BaseModel):
    id: int
    channel_comparison: Dict
    ml_predictions: Dict
    recommendations: Dict
    summary: str
    created_at: datetime
    
    class Config:
        from_attributes = True
```


#### `alert.py`

```python
from pydantic import BaseModel
from datetime import datetime

class AlertResponse(BaseModel):
    id: int
    alert_type: str
    severity: str
    channel: str
    message: str
    metric_value: float
    threshold_value: float
    is_read: bool
    created_at: datetime
    
    class Config:
        from_attributes = True
```


***

### **3. API Routers** (`app/routers/`)

–ó–∞–º–µ–Ω–∏—Ç–µ `book.py` –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ —Ä–æ—É—Ç–µ—Ä—ã:

#### `campaigns.py`

```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import List
from app.configs.database import get_db
from app.models.campaign import Campaign
from app.schemas.campaign import CampaignCreate, CampaignResponse

router = APIRouter(prefix="/api/campaigns", tags=["campaigns"])

@router.get("/", response_model=List[CampaignResponse])
def get_campaigns(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    campaigns = db.query(Campaign).offset(skip).limit(limit).all()
    return campaigns

@router.get("/{campaign_id}", response_model=CampaignResponse)
def get_campaign(campaign_id: str, db: Session = Depends(get_db)):
    campaign = db.query(Campaign).filter(Campaign.campaign_id == campaign_id).first()
    if not campaign:
        raise HTTPException(status_code=404, detail="Campaign not found")
    return campaign

@router.post("/", response_model=CampaignResponse, status_code=201)
def create_campaign(campaign: CampaignCreate, db: Session = Depends(get_db)):
    db_campaign = Campaign(**campaign.dict())
    db.add(db_campaign)
    db.commit()
    db.refresh(db_campaign)
    return db_campaign

@router.get("/channel/{channel_name}", response_model=List[CampaignResponse])
def get_campaigns_by_channel(channel_name: str, db: Session = Depends(get_db)):
    campaigns = db.query(Campaign).filter(Campaign.channel_used == channel_name).all()
    return campaigns
```


#### `analytics.py`

```python
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from sqlalchemy import func
from app.configs.database import get_db
from app.models.campaign import Campaign
from app.schemas.analytics import AnalyticsSummary, ROITrend
from app.schemas.campaign import ChannelStats
from typing import List
from datetime import datetime, timedelta

router = APIRouter(prefix="/api/analytics", tags=["analytics"])

@router.get("/summary", response_model=AnalyticsSummary)
def get_analytics_summary(db: Session = Depends(get_db)):
    """Get overall analytics summary"""
    
    # Current period stats
    total_campaigns = db.query(Campaign).count()
    current_stats = db.query(
        func.avg(Campaign.roi).label('avg_roi'),
        func.sum(Campaign.roi).label('total_roi'),
        func.avg(Campaign.conversion_rate).label('avg_conversion'),
        func.sum(Campaign.clicks).label('total_clicks'),
        func.sum(Campaign.impressions).label('total_impressions'),
        func.avg(Campaign.acquisition_cost).label('avg_cost')
    ).first()
    
    # Previous period stats (for % change calculation)
    cutoff_date = datetime.now() - timedelta(days=30)
    previous_stats = db.query(
        func.avg(Campaign.roi).label('avg_roi'),
        func.avg(Campaign.conversion_rate).label('avg_conversion'),
        func.sum(Campaign.clicks).label('total_clicks'),
        func.avg(Campaign.acquisition_cost).label('avg_cost')
    ).filter(Campaign.date < cutoff_date.date()).first()
    
    # Calculate % changes
    roi_change = ((current_stats.avg_roi - previous_stats.avg_roi) / previous_stats.avg_roi * 100) if previous_stats.avg_roi else 0
    conversion_change = ((current_stats.avg_conversion - previous_stats.avg_conversion) / previous_stats.avg_conversion * 100) if previous_stats.avg_conversion else 0
    clicks_change = ((current_stats.total_clicks - previous_stats.total_clicks) / previous_stats.total_clicks * 100) if previous_stats.total_clicks else 0
    cost_change = ((current_stats.avg_cost - previous_stats.avg_cost) / previous_stats.avg_cost * 100) if previous_stats.avg_cost else 0
    
    # Channel stats
    channel_stats = db.query(
        Campaign.channel_used,
        func.avg(Campaign.roi).label('avg_roi')
    ).group_by(Campaign.channel_used).all()
    
    channels_data = [{"channel": ch, "avg_roi": roi} for ch, roi in channel_stats]
    top_channel = max(channel_stats, key=lambda x: x[1])[0] if channel_stats else "N/A"
    worst_channel = min(channel_stats, key=lambda x: x[1])[0] if channel_stats else "N/A"
    
    return AnalyticsSummary(
        total_campaigns=total_campaigns,
        total_roi=float(current_stats.total_roi or 0),
        avg_roi=float(current_stats.avg_roi or 0),
        avg_conversion_rate=float(current_stats.avg_conversion or 0),
        total_clicks=int(current_stats.total_clicks or 0),
        total_impressions=int(current_stats.total_impressions or 0),
        avg_acquisition_cost=float(current_stats.avg_cost or 0),
        roi_change=float(roi_change),
        conversion_change=float(conversion_change),
        clicks_change=float(clicks_change),
        cost_change=float(cost_change),
        channels=channels_data,
        top_performing_channel=top_channel,
        worst_performing_channel=worst_channel
    )

@router.get("/channels", response_model=List[ChannelStats])
def get_channel_stats(db: Session = Depends(get_db)):
    """Get statistics grouped by channel"""
    
    stats = db.query(
        Campaign.channel_used,
        func.count(Campaign.id).label('total_campaigns'),
        func.avg(Campaign.roi).label('avg_roi'),
        func.avg(Campaign.conversion_rate).label('avg_conversion'),
        func.sum(Campaign.clicks).label('total_clicks'),
        func.sum(Campaign.impressions).label('total_impressions'),
        func.avg(Campaign.acquisition_cost).label('avg_cost')
    ).group_by(Campaign.channel_used).all()
    
    return [
        ChannelStats(
            channel=stat.channel_used,
            total_campaigns=stat.total_campaigns,
            avg_roi=float(stat.avg_roi),
            avg_conversion_rate=float(stat.avg_conversion),
            total_clicks=int(stat.total_clicks),
            total_impressions=int(stat.total_impressions),
            avg_acquisition_cost=float(stat.avg_cost)
        )
        for stat in stats
    ]

@router.get("/roi-trend", response_model=List[ROITrend])
def get_roi_trend(db: Session = Depends(get_db)):
    """Get ROI trend over time"""
    
    trends = db.query(
        Campaign.date,
        Campaign.channel_used,
        func.avg(Campaign.roi).label('avg_roi')
    ).group_by(Campaign.date, Campaign.channel_used).order_by(Campaign.date).all()
    
    return [
        ROITrend(
            date=str(trend.date),
            roi=float(trend.avg_roi),
            channel=trend.channel_used
        )
        for trend in trends
    ]
```


#### `predictions.py`

```python
from fastapi import APIRouter, Depends, BackgroundTasks
from sqlalchemy.orm import Session
from typing import List
from app.configs.database import get_db
from app.models.prediction import Prediction
from app.schemas.prediction import PredictionResponse
from app.services.ml_service import MLService

router = APIRouter(prefix="/api/predictions", tags=["predictions"])

@router.get("/", response_model=List[PredictionResponse])
def get_predictions(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    predictions = db.query(Prediction).order_by(Prediction.created_at.desc()).offset(skip).limit(limit).all()
    return predictions

@router.get("/latest", response_model=List[PredictionResponse])
def get_latest_predictions(db: Session = Depends(get_db)):
    """Get latest predictions for each campaign"""
    # Simplified: just get most recent 50
    predictions = db.query(Prediction).order_by(Prediction.created_at.desc()).limit(50).all()
    return predictions

@router.post("/generate")
def generate_predictions(background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """Trigger ML prediction generation for all campaigns"""
    ml_service = MLService()
    
    # Run in background
    from app.services.analytics_service import AnalyticsService
    analytics_service = AnalyticsService()
    background_tasks.add_task(analytics_service.generate_all_predictions, db)
    
    return {"message": "Prediction generation started"}
```


#### `recommendations.py`

```python
from fastapi import APIRouter, Depends, BackgroundTasks
from sqlalchemy.orm import Session
from app.configs.database import get_db
from app.models.recommendation import Recommendation
from app.schemas.recommendation import RecommendationResponse

router = APIRouter(prefix="/api/recommendations", tags=["recommendations"])

@router.get("/latest", response_model=RecommendationResponse)
def get_latest_recommendation(db: Session = Depends(get_db)):
    """Get the most recent AI recommendation"""
    recommendation = db.query(Recommendation).order_by(Recommendation.created_at.desc()).first()
    if not recommendation:
        return {"message": "No recommendations yet"}
    return recommendation

@router.post("/generate")
def generate_recommendation(background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """Trigger LLM recommendation generation"""
    from app.services.analytics_service import AnalyticsService
    analytics_service = AnalyticsService()
    background_tasks.add_task(analytics_service.generate_recommendations, db)
    
    return {"message": "Recommendation generation started"}
```


#### `alerts.py`

```python
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
from app.configs.database import get_db
from app.models.alert import Alert
from app.schemas.alert import AlertResponse

router = APIRouter(prefix="/api/alerts", tags=["alerts"])

@router.get("/", response_model=List[AlertResponse])
def get_all_alerts(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):
    alerts = db.query(Alert).order_by(Alert.created_at.desc()).offset(skip).limit(limit).all()
    return alerts

@router.get("/unread", response_model=List[AlertResponse])
def get_unread_alerts(db: Session = Depends(get_db)):
    """Get all unread alerts"""
    alerts = db.query(Alert).filter(Alert.is_read == False).order_by(Alert.created_at.desc()).all()
    return alerts

@router.patch("/{alert_id}/read")
def mark_alert_as_read(alert_id: int, db: Session = Depends(get_db)):
    """Mark an alert as read"""
    alert = db.query(Alert).filter(Alert.id == alert_id).first()
    if alert:
        alert.is_read = True
        db.commit()
        return {"message": "Alert marked as read"}
    return {"error": "Alert not found"}

@router.post("/check")
def check_for_alerts(db: Session = Depends(get_db)):
    """Manually trigger alert checking"""
    from app.services.alert_service import AlertService
    alert_service = AlertService()
    alerts = alert_service.check_alerts(db)
    return {"message": f"Generated {len(alerts)} new alerts"}
```


***

### **4. Services** (`app/services/`)

#### `ml_service.py`

```python
import pickle
import pandas as pd
import numpy as np
from typing import Dict, List
import os

class MLService:
    def __init__(self):
        model_path = os.path.join(os.path.dirname(__file__), "../../MML/model.pkl")
        if os.path.exists(model_path):
            with open(model_path, 'rb') as f:
                self.model = pickle.load(f)
        else:
            self.model = None
            print("Warning: ML model not found. Predictions will be mock.")
    
    def prepare_features(self, campaign: Dict) -> pd.DataFrame:
        """Prepare features for ML model (based on your ml.ipynb)"""
        df = pd.DataFrame([campaign])
        
        # Feature engineering
        df['CTR'] = df['clicks'] / df['impressions'] if df['impressions'].iloc[0] > 0 else 0
        df['Engagement_Rate'] = df['engagement_score'] / df['impressions'] if df['impressions'].iloc[0] > 0 else 0
        df['log_Impressions'] = np.log1p(df['impressions'])
        df['log_Clicks'] = np.log1p(df['clicks'])
        df['log_Engagement_Score'] = np.log1p(df['engagement_score'])
        
        # Date features
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            df['Year'] = df['date'].dt.year
            df['Month'] = df['date'].dt.month
            df['DayOfWeek'] = df['date'].dt.dayofweek
        
        return df
    
    def predict_roi(self, campaign_data: Dict) -> Dict:
        """Predict ROI for a campaign"""
        if self.model is None:
            # Mock prediction for MVP
            current_roi = campaign_data.get('roi', 2.0)
            predicted_roi = current_roi * (1 + np.random.uniform(-0.1, 0.2))
            confidence = np.random.uniform(0.7, 0.95)
        else:
            features = self.prepare_features(campaign_data)
            predicted_roi = self.model.predict(features)[0]
            confidence = 0.85  # Simplified
        
        risk_level = 'low' if predicted_roi > 2.0 else ('medium' if predicted_roi > 1.0 else 'high')
        
        return {
            'campaign_id': campaign_data.get('campaign_id'),
            'predicted_roi': float(predicted_roi),
            'confidence_score': float(confidence),
            'risk_level': risk_level,
            'feature_importance': {}  # Add if needed
        }
```


#### `llm_service.py`

```python
import os
import json
import requests
from typing import Dict
from dotenv import load_dotenv

load_dotenv()

class LLMService:
    def __init__(self):
        self.hf_api_key = os.getenv("HF_API_KEY")
        self.hf_api_url = "https://api-inference.huggingface.co/models/Qwen/Qwen2.5-7B-Instruct"
    
    def generate_recommendations(self, analytics_data: Dict, ml_predictions: Dict) -> Dict:
        """Generate AI recommendations"""
        
        prompt = f"""
You are a marketing analytics AI assistant. Analyze the following data and provide recommendations.

Analytics Data:
{json.dumps(analytics_data, indent=2)}

ML Predictions:
{json.dumps(ml_predictions, indent=2)}

Provide a JSON response with:
1. channel_comparison: Compare channel performance
2. ml_predictions: Interpret ML predictions
3. recommendations: Actionable recommendations for budget allocation and optimization
4. summary: Brief summary

Respond ONLY with valid JSON.
"""
        
        if not self.hf_api_key:
            # Mock response for MVP without API key
            return self._mock_recommendation(analytics_data)
        
        headers = {
            "Authorization": f"Bearer {self.hf_api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "inputs": prompt,
            "parameters": {"temperature": 0.3, "max_new_tokens": 600}
        }
        
        try:
            response = requests.post(self.hf_api_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if isinstance(data, list) and "generated_text" in data[0]:
                text = data[0]["generated_text"]
                # Extract JSON
                json_start = text.find('{')
                json_end = text.rfind('}') + 1
                if json_start != -1 and json_end > json_start:
                    return json.loads(text[json_start:json_end])
            
            return self._mock_recommendation(analytics_data)
        
        except Exception as e:
            print(f"LLM Error: {e}")
            return self._mock_recommendation(analytics_data)
    
    def _mock_recommendation(self, analytics_data: Dict) -> Dict:
        """Fallback mock recommendation for MVP"""
        return {
            "channel_comparison": {
                "top_channel": analytics_data.get('top_performing_channel', 'Instagram'),
                "analysis": "Instagram shows highest ROI. Facebook needs optimization."
            },
            "ml_predictions": {
                "trend": "ROI expected to increase by 15% with current strategy"
            },
            "recommendations": {
                "budget_allocation": f"Increase budget for {analytics_data.get('top_performing_channel', 'Instagram')} by 20%",
                "optimization": "Reduce spend on underperforming channels"
            },
            "summary": "Overall performance is positive. Focus on top channels and optimize creatives."
        }
```


#### `alert_service.py`

```python
from typing import List, Dict
from datetime import datetime, timedelta
from sqlalchemy.orm import Session
from app.models.alert import Alert
from app.models.campaign import Campaign

class AlertService:
    THRESHOLDS = {
        'roi_drop': 1.5,
        'cost_spike': 600,
        'low_conversion': 0.05,
        'engagement_drop': 3
    }
    
    def check_alerts(self, db: Session) -> List[Dict]:
        """Check for alert conditions"""
        alerts = []
        
        # Get recent campaigns
        recent = db.query(Campaign).filter(
            Campaign.updated_at >= datetime.now() - timedelta(hours=24)
        ).all()
        
        for campaign in recent:
            # ROI drop
            if campaign.roi < self.THRESHOLDS['roi_drop']:
                alerts.append(self._create_alert(
                    'roi_drop', 'high', campaign.channel_used,
                    f"ROI dropped to {campaign.roi:.2f} for {campaign.channel_used}",
                    campaign.roi, self.THRESHOLDS['roi_drop']
                ))
            
            # Cost spike
            if campaign.acquisition_cost > self.THRESHOLDS['cost_spike']:
                alerts.append(self._create_alert(
                    'cost_spike', 'medium', campaign.channel_used,
                    f"Cost spiked to ${campaign.acquisition_cost} for {campaign.channel_used}",
                    campaign.acquisition_cost, self.THRESHOLDS['cost_spike']
                ))
            
            # Low conversion
            if campaign.conversion_rate < self.THRESHOLDS['low_conversion']:
                alerts.append(self._create_alert(
                    'low_conversion', 'medium', campaign.channel_used,
                    f"Low conversion {campaign.conversion_rate:.2%} for {campaign.channel_used}",
                    campaign.conversion_rate, self.THRESHOLDS['low_conversion']
                ))
        
        # Save to DB
        for alert_data in alerts:
            alert = Alert(**alert_data)
            db.add(alert)
        db.commit()
        
        return alerts
    
    def _create_alert(self, alert_type, severity, channel, message, value, threshold):
        return {
            'alert_type': alert_type,
            'severity': severity,
            'channel': channel,
            'message': message,
            'metric_value': value,
            'threshold_value': threshold,
            'is_read': False
        }
```


#### `analytics_service.py`

```python
from sqlalchemy.orm import Session
from app.models.campaign import Campaign
from app.models.prediction import Prediction
from app.models.recommendation import Recommendation
from app.services.ml_service import MLService
from app.services.llm_service import LLMService
from typing import List, Dict

class AnalyticsService:
    def get_recent_campaigns(self, db: Session) -> List[Dict]:
        """Get recent campaigns for prediction"""
        campaigns = db.query(Campaign).limit(100).all()
        return [
            {
                'campaign_id': c.campaign_id,
                'clicks': c.clicks,
                'impressions': c.impressions,
                'engagement_score': c.engagement_score,
                'conversion_rate': c.conversion_rate,
                'roi': c.roi,
                'date': c.date
            }
            for c in campaigns
        ]
    
    def generate_all_predictions(self, db: Session):
        """Generate predictions for all campaigns"""
        ml_service = MLService()
        campaigns = self.get_recent_campaigns(db)
        
        for campaign_data in campaigns:
            prediction = ml_service.predict_roi(campaign_data)
            db_prediction = Prediction(**prediction)
            db.add(db_prediction)
        
        db.commit()
    
    def generate_recommendations(self, db: Session):
        """Generate LLM recommendations"""
        llm_service = LLMService()
        
        # Get analytics data
        from app.routers.analytics import get_analytics_summary
        analytics = get_analytics_summary(db)
        
        # Get recent predictions
        predictions = db.query(Prediction).order_by(Prediction.created_at.desc()).limit(10).all()
        ml_data = {
            'recent_predictions': [
                {'campaign_id': p.campaign_id, 'predicted_roi': p.predicted_roi, 'risk': p.risk_level}
                for p in predictions
            ]
        }
        
        # Generate recommendations
        result = llm_service.generate_recommendations(analytics.dict(), ml_data)
        
        # Save to DB
        rec = Recommendation(
            channel_comparison=result.get('channel_comparison', {}),
            ml_predictions=result.get('ml_predictions', {}),
            recommendations=result.get('recommendations', {}),
            summary=result.get('summary', '')
        )
        db.add(rec)
        db.commit()
```


***

### **5. Scheduler** (`app/scheduler/`)

Create `tasks.py`:

```python
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
from app.configs.database import SessionLocal
from app.services.analytics_service import AnalyticsService
from app.services.alert_service import AlertService
import logging

logger = logging.getLogger(__name__)

def hourly_refresh():
    """Hourly refresh task"""
    logger.info("Starting hourly refresh...")
    db = SessionLocal()
    
    try:
        analytics_service = AnalyticsService()
        alert_service = AlertService()
        
        # Generate predictions
        analytics_service.generate_all_predictions(db)
        logger.info("Predictions updated")
        
        # Generate recommendations
        analytics_service.generate_recommendations(db)
        logger.info("Recommendations updated")
        
        # Check alerts
        alerts = alert_service.check_alerts(db)
        logger.info(f"Generated {len(alerts)} alerts")
        
    except Exception as e:
        logger.error(f"Error in hourly refresh: {e}")
    finally:
        db.close()

def start_scheduler():
    """Start background scheduler"""
    scheduler = BackgroundScheduler()
    
    # Every hour
    scheduler.add_job(
        hourly_refresh,
        trigger=IntervalTrigger(hours=1),
        id='hourly_refresh',
        replace_existing=True
    )
    
    # Run on startup
    scheduler.add_job(hourly_refresh, id='startup_refresh')
    
    scheduler.start()
    logger.info("Scheduler started")
```


***

### **6. Update `main.py`**

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from app.configs.database import engine, Base
from app.routers import campaigns, analytics, predictions, recommendations, alerts
from app.scheduler.tasks import start_scheduler
import logging

# Create tables
Base.metadata.create_all(bind=engine)

app = FastAPI(title="DEMETRA SYSTEMS - AI Analytics API", version="1.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(campaigns.router)
app.include_router(analytics.router)
app.include_router(predictions.router)
app.include_router(recommendations.router)
app.include_router(alerts.router)

# Start scheduler
@app.on_event("startup")
def startup_event():
    logging.basicConfig(level=logging.INFO)
    start_scheduler()
    logging.info("Application started")

@app.get("/")
def read_root():
    return {"message": "DEMETRA SYSTEMS AI Analytics API", "version": "1.0.0"}

@app.get("/health")
def health_check():
    return {"status": "healthy"}
```


***

### **7. Data Seeder** (`app/utils/seeder.py`)

```python
import pandas as pd
from sqlalchemy.orm import Session
from app.models.campaign import Campaign
from app.configs.database import SessionLocal
import os

def seed_campaigns_from_csv():
    """Load CSV data into database"""
    db = SessionLocal()
    
    csv_path = os.path.join(os.path.dirname(__file__), "../../MML/data/Social_Media_Advertising.csv")
    
    if not os.path.exists(csv_path):
        print("CSV file not found!")
        return
    
    df = pd.read_csv(csv_path)
    df['Date'] = pd.to_datetime(df['Date'])
    
    for _, row in df.iterrows():
        campaign = Campaign(
            campaign_id=str(row['Campaign_ID']),
            target_audience=row['Target_Audience'],
            campaign_goal=row['Campaign_Goal'],
            duration=row['Duration'],
            channel_used=row['Channel_Used'],
            conversion_rate=float(row['Conversion_Rate']),
            acquisition_cost=float(row['Acquisition_Cost'].replace('$', '').replace(',', '')),
            roi=float(row['ROI']),
            location=row['Location'],
            language=row['Language'],
            clicks=int(row['Clicks']),
            impressions=int(row['Impressions']),
            engagement_score=int(row['Engagement_Score']),
            customer_segment=row['Customer_Segment'],
            date=row['Date'].date()
        )
        db.add(campaign)
    
    db.commit()
    db.close()
    print(f"Seeded {len(df)} campaigns!")

if __name__ == "__main__":
    seed_campaigns_from_csv()
```


***

## üìã pyproject.toml Dependencies

Add these to your `pyproject.toml`:

```toml
[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.104.0"
uvicorn = {extras = ["standard"], version = "^0.24.0"}
sqlalchemy = "^2.0.0"
psycopg2-binary = "^2.9.9"
pydantic = "^2.4.0"
python-dotenv = "^1.0.0"
pandas = "^2.1.0"
numpy = "^1.26.0"
scikit-learn = "^1.3.0"
requests = "^2.31.0"
apscheduler = "^3.10.4"
```


***

## üéØ MVP Implementation Checklist

**Phase 1 (Day 1):**

- [ ] Create all 4 models (campaign, prediction, recommendation, alert)
- [ ] Create all schemas
- [ ] Implement campaigns router (CRUD)
- [ ] Implement analytics router (summary, channels, roi-trend)
- [ ] Run seeder to load CSV data

**Phase 2 (Day 2):**

- [ ] Implement ML service (use your ml.ipynb model)
- [ ] Implement LLM service (use your recommendations.py)
- [ ] Implement predictions router
- [ ] Implement recommendations router
- [ ] Test ML + LLM integration

**Phase 3 (Day 3):**

- [ ] Implement alert service
- [ ] Implement alerts router
- [ ] Implement scheduler (APScheduler)
- [ ] Test hourly refresh
- [ ] Final testing + bug fixes

***

–≠—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π backend –¥–ª—è MVP. –í—Å–µ –≥–æ—Ç–æ–≤–æ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –≤–∞—à–∏–º–∏ ML –∏ LLM –º–æ–¥–µ–ª—è–º–∏! –ù—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —á–∞—Å—Ç—å—é?

